{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9450ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "import os, json\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e55347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_uni = {}\n",
    "for letter in \"abcdefghijklmnopqrstuvwxyzαβγδεζηθ\":\n",
    "    str_to_uni[letter] = ord(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea896b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Original text length = 377\n",
      "----------\n",
      "Tokenized length = 387\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Most devs love jumping straight into code. I used to do that too. You open your editor, spin up a Next.js app, and you’re off.\n",
    "Feels good — until it doesn’t. Because after a few weeks, you realize your files are everywhere. APIs live in one folder,\n",
    "components in another, and no one knows what’s going on. That’s when I started doing something different. I call it Vibe Coding.\"\"\"\n",
    "tokens = text.encode(\"utf-8\")\n",
    "\n",
    "print(\"-\"*10)\n",
    "print(\"Original text length =\",len(text))\n",
    "print(\"-\"*10)\n",
    "print(\"Tokenized length =\",len(tokens))\n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fd38c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinations(tokens):\n",
    "    counts = {}\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return dict(sorted(counts.items(), key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e362dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = get_combinations(tokens)\n",
    "max(counts, key=counts.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eba9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(tokens, pair, index):\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "            new_tokens.append(index)\n",
    "            i+=2\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i+=1\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4071baf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge([1,2,2,3,4,5], (1,2), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e64a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./shakespeare.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff131e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging pair f(32, 32) into new token 256\n",
      "Merging pair f(256, 256) into new token 257\n",
      "Merging pair f(101, 32) into new token 258\n",
      "Merging pair f(116, 104) into new token 259\n",
      "Merging pair f(10, 257) into new token 260\n",
      "Merging pair f(116, 32) into new token 261\n",
      "Merging pair f(115, 32) into new token 262\n",
      "Merging pair f(44, 32) into new token 263\n",
      "Merging pair f(100, 32) into new token 264\n",
      "Merging pair f(111, 117) into new token 265\n",
      "Merging pair f(101, 114) into new token 266\n",
      "Merging pair f(105, 110) into new token 267\n",
      "Merging pair f(97, 110) into new token 268\n",
      "Merging pair f(121, 32) into new token 269\n",
      "Merging pair f(46, 32) into new token 270\n",
      "Merging pair f(111, 114) into new token 271\n",
      "Merging pair f(10, 256) into new token 272\n",
      "Merging pair f(111, 32) into new token 273\n",
      "Merging pair f(101, 110) into new token 274\n",
      "Merging pair f(97, 114) into new token 275\n",
      "Merging pair f(32, 259) into new token 276\n",
      "Merging pair f(108, 108) into new token 277\n",
      "Merging pair f(111, 110) into new token 278\n",
      "Merging pair f(104, 97) into new token 279\n",
      "Merging pair f(257, 257) into new token 280\n",
      "Merging pair f(105, 262) into new token 281\n",
      "Merging pair f(101, 115) into new token 282\n",
      "Merging pair f(44, 260) into new token 283\n",
      "Merging pair f(121, 265) into new token 284\n",
      "Merging pair f(268, 264) into new token 285\n",
      "Merging pair f(46, 272) into new token 286\n",
      "Merging pair f(73, 32) into new token 287\n",
      "Merging pair f(111, 102) into new token 288\n",
      "Merging pair f(101, 97) into new token 289\n",
      "Merging pair f(277, 32) into new token 290\n",
      "Merging pair f(116, 273) into new token 291\n",
      "Merging pair f(111, 119) into new token 292\n",
      "Merging pair f(119, 105) into new token 293\n",
      "Merging pair f(266, 32) into new token 294\n",
      "Merging pair f(267, 103) into new token 295\n",
      "Merging pair f(111, 109) into new token 296\n",
      "Merging pair f(114, 32) into new token 297\n",
      "Merging pair f(115, 116) into new token 298\n",
      "Merging pair f(32, 109) into new token 299\n",
      "Merging pair f(259, 258) into new token 300\n",
      "Merging pair f(99, 104) into new token 301\n",
      "Merging pair f(32, 104) into new token 302\n",
      "Merging pair f(110, 111) into new token 303\n",
      "Merging pair f(118, 258) into new token 304\n",
      "Merging pair f(115, 101) into new token 305\n",
      "Merging pair f(84, 104) into new token 306\n",
      "Merging pair f(102, 271) into new token 307\n",
      "Merging pair f(97, 32) into new token 308\n",
      "Merging pair f(114, 101) into new token 309\n",
      "Merging pair f(276, 258) into new token 310\n",
      "Merging pair f(108, 105) into new token 311\n",
      "Merging pair f(46, 260) into new token 312\n",
      "Merging pair f(97, 261) into new token 313\n",
      "Merging pair f(101, 263) into new token 314\n",
      "Merging pair f(288, 32) into new token 315\n",
      "Merging pair f(259, 32) into new token 316\n",
      "Merging pair f(105, 109) into new token 317\n",
      "Merging pair f(115, 104) into new token 318\n",
      "Merging pair f(111, 111) into new token 319\n",
      "Merging pair f(105, 116) into new token 320\n",
      "Merging pair f(101, 275) into new token 321\n",
      "Merging pair f(97, 116) into new token 322\n",
      "Merging pair f(103, 104) into new token 323\n",
      "Merging pair f(65, 110) into new token 324\n",
      "Merging pair f(59, 32) into new token 325\n",
      "Merging pair f(115, 261) into new token 326\n",
      "Merging pair f(108, 101) into new token 327\n",
      "Merging pair f(114, 105) into new token 328\n",
      "Merging pair f(114, 97) into new token 329\n",
      "Merging pair f(303, 261) into new token 330\n",
      "Merging pair f(284, 32) into new token 331\n",
      "Merging pair f(280, 280) into new token 332\n",
      "Merging pair f(295, 32) into new token 333\n",
      "Merging pair f(109, 269) into new token 334\n",
      "Merging pair f(101, 101) into new token 335\n",
      "Merging pair f(39, 262) into new token 336\n",
      "Merging pair f(324, 264) into new token 337\n",
      "Merging pair f(97, 109) into new token 338\n",
      "Merging pair f(117, 110) into new token 339\n",
      "Merging pair f(108, 264) into new token 340\n",
      "Merging pair f(59, 260) into new token 341\n",
      "Merging pair f(267, 32) into new token 342\n",
      "Merging pair f(279, 261) into new token 343\n",
      "Merging pair f(117, 261) into new token 344\n",
      "Merging pair f(97, 262) into new token 345\n",
      "Merging pair f(98, 101) into new token 346\n",
      "Merging pair f(119, 104) into new token 347\n",
      "Merging pair f(115, 105) into new token 348\n",
      "Merging pair f(274, 32) into new token 349\n",
      "Merging pair f(97, 108) into new token 350\n",
      "Merging pair f(69, 82) into new token 351\n",
      "Merging pair f(100, 105) into new token 352\n",
      "Merging pair f(101, 262) into new token 353\n",
      "Merging pair f(117, 114) into new token 354\n",
      "Merging pair f(39, 264) into new token 355\n"
     ]
    }
   ],
   "source": [
    "num_merges = 100\n",
    "ids = list(tokens)\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    combinations = get_combinations(ids)\n",
    "    top_pair = max(combinations, key=combinations.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"Merging pair f{top_pair} into new token {idx}\")\n",
    "    ids = merge(ids,top_pair,idx)\n",
    "    merges[top_pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d316f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial tokens length = 5436475\n",
      "New tokens length = 3175423\n",
      "Compression ratio = 1.71 X\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial tokens length = {len(tokens)}\")\n",
    "print(f\"New tokens length = {len(ids)}\")\n",
    "print(f\"Compression ratio = {len(tokens)/len(ids):.2f} X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8cff2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1] \n",
    "    \n",
    "def decode(input: list[int]) -> str:\n",
    "    tokens = b\"\".join(vocab[idx] for idx in input)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "    \n",
    "def encode(input: str) -> list[int]:\n",
    "    tokens = list(input.encode(\"utf-8\"))\n",
    "    for (token_1, token_2), value in merges.items():\n",
    "        tokens = merge(tokens, (token_1, token_2), value)\n",
    "    \n",
    "    return tokens \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094b0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  This is a test  \n"
     ]
    }
   ],
   "source": [
    "encoded_tokens = encode(\"This is a test\")\n",
    "decoded_tokens = decode(encoded_tokens)\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./gpt2_vocab.json\", \"r\") as vocab:\n",
    "    encoder = json.load(vocab)\n",
    "encoder[\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba36b0e",
   "metadata": {},
   "source": [
    "You can find more info regarding gpt-2 and other variants in [Tiktoken Openai Tokenizers](https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bb79935",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_regex = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}++|\\p{N}{1,3}+| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*+|\\s++$|\\s*[\\r\\n]|\\s+(?!\\S)|\\s\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1151dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ' world',\n",
       " ',',\n",
       " ' today',\n",
       " ' we',\n",
       " ' are',\n",
       " ' going',\n",
       " ' to',\n",
       " ' learn',\n",
       " ' about',\n",
       " ' OpenAI',\n",
       " ' gpt',\n",
       " ' tokenizers']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(tokenizer_regex, \"Hello world, today we are going to learn about OpenAI gpt tokenizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6db945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awesome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
