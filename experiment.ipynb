{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9450ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "import os, json\n",
    "from vbtokenizer import SimpleTokenizer\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e55347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_uni = {}\n",
    "for letter in \"abcdefghijklmnopqrstuvwxyzαβγδεζηθ\":\n",
    "    str_to_uni[letter] = ord(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea896b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Original text length = 377\n",
      "----------\n",
      "Tokenized length = 387\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Most devs love jumping straight into code. I used to do that too. You open your editor, spin up a Next.js app, and you’re off.\n",
    "Feels good — until it doesn’t. Because after a few weeks, you realize your files are everywhere. APIs live in one folder,\n",
    "components in another, and no one knows what’s going on. That’s when I started doing something different. I call it Vibe Coding.\"\"\"\n",
    "tokens = text.encode(\"utf-8\")\n",
    "\n",
    "print(\"-\"*10)\n",
    "print(\"Original text length =\",len(text))\n",
    "print(\"-\"*10)\n",
    "print(\"Tokenized length =\",len(tokens))\n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fd38c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinations(tokens):\n",
    "    counts = {}\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return dict(sorted(counts.items(), key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e362dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = get_combinations(tokens)\n",
    "max(counts, key=counts.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eba9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(tokens, pair, index):\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "            new_tokens.append(index)\n",
    "            i+=2\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i+=1\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4071baf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge([1,2,2,3,4,5], (1,2), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e64a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./shakespeare.txt', 'r') as file:\n",
    "    shakespeare_text = file.read()\n",
    "tokens = shakespeare_text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff131e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging pair f(32, 32) into new token 256\n",
      "Merging pair f(256, 256) into new token 257\n"
     ]
    }
   ],
   "source": [
    "num_merges = 2\n",
    "ids = list(tokens)\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    combinations = get_combinations(ids)\n",
    "    top_pair = max(combinations, key=combinations.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"Merging pair f{top_pair} into new token {idx}\")\n",
    "    ids = merge(ids,top_pair,idx)\n",
    "    merges[top_pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d316f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial tokens length = 5436475\n",
      "New tokens length = 5085353\n",
      "Compression ratio = 1.07 X\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial tokens length = {len(tokens)}\")\n",
    "print(f\"New tokens length = {len(ids)}\")\n",
    "print(f\"Compression ratio = {len(tokens)/len(ids):.2f} X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cff2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1] \n",
    "    \n",
    "def decode(input: list[int]) -> str:\n",
    "    tokens = b\"\".join(vocab[idx] for idx in input)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "    \n",
    "def encode(input: str) -> list[int]:\n",
    "    tokens = list(input.encode(\"utf-8\"))\n",
    "    for (token_1, token_2), value in merges.items():\n",
    "        tokens = merge(tokens, (token_1, token_2), value)\n",
    "    \n",
    "    return tokens \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3094b0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test\n"
     ]
    }
   ],
   "source": [
    "encoded_tokens = encode(\"This is a test\")\n",
    "decoded_tokens = decode(encoded_tokens)\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b38a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./gpt2_merges.txt\", \"r\") as bpe:\n",
    "    bpe_data = bpe.read().split(\"\\n\")[1:-1]\n",
    "bpe_merges = [tuple(merge.split()) for merge in bpe_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4169bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./gpt2_vocab.json\", \"r\") as vocab:\n",
    "    encoder = json.load(vocab)\n",
    "encoder[\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba36b0e",
   "metadata": {},
   "source": [
    "You can find more info regarding gpt-2 and other variants in [Tiktoken Openai Tokenizers](https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bb79935",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_regex = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}++|\\p{N}{1,3}+| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*+|\\s++$|\\s*[\\r\\n]|\\s+(?!\\S)|\\s\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1151dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ' world',\n",
       " ',',\n",
       " ' today',\n",
       " ' we',\n",
       " ' are',\n",
       " ' going',\n",
       " ' to',\n",
       " ' learn',\n",
       " ' about',\n",
       " ' OpenAI',\n",
       " ' gpt',\n",
       " ' tokenizers']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(tokenizer_regex, \"Hello world, today we are going to learn about OpenAI gpt tokenizers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e6e7b",
   "metadata": {},
   "source": [
    "## Base Tokenizer\n",
    "\n",
    "Do the same as before but more cleanly using the class we built BaseTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc6db945",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eca498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging byte pairs: 100%|██████████| 4/4 [00:09<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(shakespeare_text, 260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08179a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided sentence: Hello there Vasilis\n",
      "Tokenized sentence = [72, 101, 108, 108, 111, 32, 259, 101, 114, 258, 86, 97, 115, 105, 108, 105, 115]\n",
      "Decoded sentence = Hello there Vasilis\n"
     ]
    }
   ],
   "source": [
    "initial_sentence = \"Hello there Vasilis\"\n",
    "print(f\"Provided sentence: {initial_sentence}\")\n",
    "tokenized_input = tokenizer.encode(initial_sentence)\n",
    "print(\"Tokenized sentence =\",tokenized_input)\n",
    "decoded_input = tokenizer.decode(tokenized_input)\n",
    "print(\"Decoded sentence =\",decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "for special_token in [\"<vm_start>\", \"<vm_user>\",\"<vm_assistant>\",\"<vm_end>\"]:\n",
    "    tokenizer.add_special_token(special_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48e8d797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;1m✘ Skipping: The `<vm_start>` special token already exists\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_token(\"<vm_start>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75821743",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"my_first_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b7d5ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = SimpleTokenizer()\n",
    "tokenizer2.load(\"my_first_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5214a61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(32, 32): 256, (256, 256): 257, (101, 32): 258, (116, 104): 259}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39f581cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(32, 32): 256, (256, 256): 257, (101, 32): 258, (116, 104): 259}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0419d875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence with tokenizer_2 = [72, 101, 108, 108, 111, 32, 259, 101, 114, 258, 86, 97, 115, 105, 108, 105, 115]\n",
      "Tokenized list match with saved tokenizer True\n",
      "Decoded sentence with tokenizer_2 = Hello there Vasilis\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_2 = tokenizer2.encode(initial_sentence)\n",
    "print(\"Tokenized sentence with tokenizer_2 =\",tokenized_input_2)\n",
    "print(\"Tokenized list match with saved tokenizer\", tokenized_input_2 == tokenized_input)\n",
    "decoded_input_2 = tokenizer2.decode(tokenized_input_2)\n",
    "print(\"Decoded sentence with tokenizer_2 =\",decoded_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddc75752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded decoded sentence result = <vm_start> <vm_user>Does the special tokens work? <vm_assistant>Yes they do work! <vm_end>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer2.encode(\"<vm_start> <vm_user>Does the special tokens work? <vm_assistant>Yes they do work! <vm_end>\")\n",
    "print(f\"Encoded decoded sentence result = {tokenizer2.decode(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841af1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awesome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
